{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c17afc91",
   "metadata": {
    "id": "c17afc91"
   },
   "source": [
    "# Custom Chatbot Project\n",
    "\n",
    "# Dataset Selection\n",
    "\n",
    "In this project, I am building a custom chatbot by leveraging OpenAI's language models and a custom dataset. The dataset chosen is *character_descriptions.csv*, which contains structured information about various fictional characters including their name, description, medium, and setting.\n",
    "\n",
    "### Why this dataset?\n",
    "\n",
    "This dataset is ideal for building a chatbot that can answer questions about different characters. The structured fields allow us to generate meaningful narratives for embeddings and support a variety of character-specific queries. Additionally, this dataset includes more than 20 rows, satisfying the requirement for a rich context base."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff0f54",
   "metadata": {
    "id": "10ff0f54"
   },
   "source": [
    "# Sample Prompt Creation\n",
    "\n",
    "I randomly selected two characters from the dataset and use their names to create basic natural language questions. This simulates user input and helps test the chatbot's capabilities early in the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d4c5f",
   "metadata": {
    "id": "a63d4c5f"
   },
   "source": [
    "# Data Wrangling\n",
    "\n",
    "## Setting up OpenAI API\n",
    "Sets the API base to the custom endpoint provided by Vocareum (used in Udacity's workspace) and initializes the API key to access OpenAI's services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69b83a1",
   "metadata": {
    "id": "c69b83a1"
   },
   "outputs": [],
   "source": [
    "import openai\n",
    "openai.api_base=\"https://openai.vocareum.com/v1\"\n",
    "openai.api_key=\"YOUR API KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2Gn-HYrK6Jwo",
   "metadata": {
    "id": "2Gn-HYrK6Jwo"
   },
   "source": [
    "## Loading and Processing Dataset\n",
    "Loads the dataset and combines relevant character details into a single text column (required for embedding and RAG logic). Keeps only the text column to simplify later processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a595980",
   "metadata": {
    "id": "0a595980"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path='data/character_descriptions.csv'\n",
    "character_data=pd.read_csv(file_path)\n",
    "\n",
    "def combine_columns(row):\n",
    "    text = f\"{row['Name']} is a {row['Description']} This character appears in a {row['Medium']} set in {row['Setting']}.\"\n",
    "    return text\n",
    "\n",
    "character_data['text']=character_data.apply(combine_columns, axis=1)\n",
    "combined_df=character_data[['text']]\n",
    "combined_df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-K-XoJ_I6VeJ",
   "metadata": {
    "id": "-K-XoJ_I6VeJ"
   },
   "source": [
    "## Generating Sample Prompts and Running Basic Q&A\n",
    "Randomly samples two characters from the dataset, creates prompt questions using their names, and retrieves answers using OpenAI’s gpt-3.5-turbo-instruct. This represents a basic query, without custom context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb3a9fd",
   "metadata": {
    "id": "acb3a9fd"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Randomly select two rows from the dataframe\n",
    "sampled_rows=combined_df.sample(2).reset_index(drop=True)\n",
    "\n",
    "character_1=sampled_rows.iloc[0]['text'].split()[0]\n",
    "character_2=sampled_rows.iloc[1]['text'].split()[0]\n",
    "\n",
    "# Generate two questions based on the selected characters\n",
    "prompt1=f\"What is {character_1}'s profession?\"\n",
    "prompt2=f\"In what setting does {character_2}'s story take place?\"\n",
    "\n",
    "print(f'Prompt1: {prompt1}')\n",
    "\n",
    "answer1 = openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt1,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(answer1)\n",
    "\n",
    "print(f'Prompt2: {prompt2}')\n",
    "\n",
    "answer2=openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=prompt2,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xuoFmQfE6us0",
   "metadata": {
    "id": "xuoFmQfE6us0"
   },
   "source": [
    "## Embedding Generation\n",
    "\n",
    "Here, we use OpenAI’s `text-embedding-ada-002` model to create dense vector representations (embeddings) of each text entry. These embeddings capture semantic meaning and are later used to determine relevance during querying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pemSKOFL6mkT",
   "metadata": {
    "id": "pemSKOFL6mkT"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_MODEL_NAME=\"text-embedding-ada-002\"\n",
    "batch_size=100\n",
    "embeddings=[]\n",
    "for i in range(0, len(combined_df), batch_size):\n",
    "    response = openai.Embedding.create(\n",
    "        input=combined_df.iloc[i:i+batch_size][\"text\"].tolist(),\n",
    "        engine=EMBEDDING_MODEL_NAME\n",
    "    )\n",
    "\n",
    "    embeddings.extend([data[\"embedding\"] for data in response[\"data\"]])\n",
    "\n",
    "combined_df[\"embeddings\"]=embeddings\n",
    "combined_df\n",
    "\n",
    "combined_df.to_csv('character_descriptions_with_embeddings.csv')\n",
    "len(combined_df['embeddings'][0])\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae769871",
   "metadata": {
    "id": "ae769871"
   },
   "source": [
    "# Custom Query Completion\n",
    "\n",
    "## Load CSV with Embeddings\n",
    "Reloads the CSV with saved embeddings and converts the text-form embeddings back into NumPy arrays for distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582f0656",
   "metadata": {
    "id": "582f0656"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path='character_descriptions_with_embeddings.csv'\n",
    "df=pd.read_csv(file_path, index_col=0)\n",
    "df[\"embeddings\"]=df[\"embeddings\"].apply(eval).apply(np.array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w4sCUQ887R4j",
   "metadata": {
    "id": "w4sCUQ887R4j"
   },
   "source": [
    "## Define get_rows_sorted_by_relevance()\n",
    "Calculates cosine similarity between the embedding of a user’s question and each row’s text embedding. Returns the dataframe sorted from most relevant to least relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TDYGb0QM7Lgi",
   "metadata": {
    "id": "TDYGb0QM7Lgi"
   },
   "outputs": [],
   "source": [
    "from openai.embeddings_utils import get_embedding, distances_from_embeddings\n",
    "\n",
    "def get_rows_sorted_by_relevance(question, df):\n",
    "    \"\"\"\n",
    "    Function that takes in a question string and a dataframe containing\n",
    "    rows of text and associated embeddings, and returns that dataframe\n",
    "    sorted from least to most relevant for that question\n",
    "    \"\"\"\n",
    "\n",
    "    question_embeddings=get_embedding(question, engine=EMBEDDING_MODEL_NAME)\n",
    "\n",
    "    df_copy=df.copy()\n",
    "    df_copy[\"distances\"]=distances_from_embeddings(\n",
    "        question_embeddings,\n",
    "        df_copy[\"embeddings\"].values,\n",
    "        distance_metric=\"cosine\"\n",
    "    )\n",
    "\n",
    "    df_copy.sort_values(\"distances\", ascending=True, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rvr4vgGt7ZTO",
   "metadata": {
    "id": "rvr4vgGt7ZTO"
   },
   "source": [
    "## Check shape of first embedding\n",
    "Verifies that each embedding is a fixed-size vector (typically 1536 dimensions for text-embedding-ada-002)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6e1f75",
   "metadata": {
    "id": "8b6e1f75"
   },
   "outputs": [],
   "source": [
    "df['embeddings'][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e9fb99",
   "metadata": {
    "id": "a2e9fb99"
   },
   "source": [
    "# Custom Query Completion\n",
    "\n",
    "Demonstrates relevance-based retrieval. Retrieves the top few character descriptions most relevant to the input question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f2dcd7",
   "metadata": {
    "id": "13f2dcd7"
   },
   "outputs": [],
   "source": [
    "question1=\"What is Malvolio's profession?\"\n",
    "sorted_df1=get_rows_sorted_by_relevance(question1, df)\n",
    "sorted_df1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c403f543",
   "metadata": {
    "id": "c403f543"
   },
   "outputs": [],
   "source": [
    "question2=\"In what setting does Karma's story take place?\"\n",
    "sorted_df2=get_rows_sorted_by_relevance(question2, df)\n",
    "sorted_df2.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fz4EUa817yW4",
   "metadata": {
    "id": "fz4EUa817yW4"
   },
   "source": [
    "## Prompt Engineering Logic\n",
    "\n",
    "Builds a prompt that combines multiple relevant text chunks into a single context, ensuring that the total token count stays within model limits. This prepares input for a context-aware custom query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74280b92",
   "metadata": {
    "id": "74280b92"
   },
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "prompt_template=\"\"\"\n",
    "Answer the question based on the context given below, and if the question\n",
    "is unanswerable or not relevant to the provided data, just say \"Sorry. I don't know. Please provide some more data.\"\n",
    "\n",
    "Context:\n",
    "\n",
    "{}\n",
    "\n",
    "***************************\n",
    "\n",
    "Question: {}\n",
    "Answer:\"\"\"\n",
    "\n",
    "\n",
    "def create_prompt(question, df, max_token_count, prompt_template=prompt_template):\n",
    "    \"\"\"\n",
    "    Given a question and a dataframe containing rows of text and their\n",
    "    embeddings, return a text prompt to send to a Completion model\n",
    "    \"\"\"\n",
    "    # Create a tokenizer that is designed to align with our embeddings\n",
    "    tokenizer=tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "\n",
    "    current_token_count=len(tokenizer.encode(prompt_template)) + \\\n",
    "                            len(tokenizer.encode(question))\n",
    "\n",
    "    context=[]\n",
    "    for text in get_rows_sorted_by_relevance(question, df)[\"text\"].values:\n",
    "\n",
    "        # Increase the counter based on the number of tokens in this row\n",
    "        text_token_count=len(tokenizer.encode(text))\n",
    "        current_token_count+=text_token_count\n",
    "\n",
    "        # Add the row of text to the list if we haven't exceeded the max\n",
    "        if current_token_count<=max_token_count:\n",
    "            context.append(text)\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    return prompt_template.format(\"\\n\\n--------------------------------------------------\\n\\n\".join(context), question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Z48RdX408KRF",
   "metadata": {
    "id": "Z48RdX408KRF"
   },
   "source": [
    "## Show Prompts for 2 Questions\n",
    "\n",
    "Displays the final custom prompt generated for the questions, showing how the model will be guided to use context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "H8RSUCe98Fx2",
   "metadata": {
    "id": "H8RSUCe98Fx2"
   },
   "outputs": [],
   "source": [
    "# create prompt for question 1\n",
    "max_token_count=300\n",
    "print(create_prompt(question1, df, max_token_count))\n",
    "\n",
    "\n",
    "# create prompt for question 2\n",
    "max_token_count=300\n",
    "print(create_prompt(question2, df, max_token_count))\n",
    "\n",
    "COMPLETION_MODEL_NAME=\"gpt-3.5-turbo-instruct\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "xEP6Jw678Tuv",
   "metadata": {
    "id": "xEP6Jw678Tuv"
   },
   "source": [
    "## Define Final Answering Function\n",
    "\n",
    "Runs a Completion query using a prompt created by create_prompt(). This is the core function for producing answers with the custom RAG-like system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KtB45eJM8ScX",
   "metadata": {
    "id": "KtB45eJM8ScX"
   },
   "outputs": [],
   "source": [
    "def answer_question(\n",
    "    question, df, max_prompt_tokens=1800, max_answer_tokens=150\n",
    "):\n",
    "    \"\"\"\n",
    "    Given a question, a dataframe containing rows of text, and a maximum\n",
    "    number of desired tokens in the prompt and response,\n",
    "    Return the answer to the question according to an OpenAI Completion model.\n",
    "    If the model produces an error, return an empty string.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt=create_prompt(question, df, max_prompt_tokens)\n",
    "\n",
    "    try:\n",
    "        response=openai.Completion.create(\n",
    "            model=COMPLETION_MODEL_NAME,\n",
    "            prompt=prompt,\n",
    "            max_tokens=max_answer_tokens\n",
    "        )\n",
    "        return response[\"choices\"][0][\"text\"].strip()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return \"\"\n",
    "\n",
    "answer1=answer_question(question1, df)\n",
    "print(answer1)\n",
    "\n",
    "answer2=answer_question(question2, df)\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1783f146",
   "metadata": {
    "id": "1783f146"
   },
   "source": [
    "# Custom Performance Demonstration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f11fdc0",
   "metadata": {
    "id": "4f11fdc0"
   },
   "source": [
    "## Basic completion for question 1\n",
    "\n",
    "Asks a basic question with no custom context. This shows how the model performs without dataset guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901c850",
   "metadata": {
    "id": "4901c850"
   },
   "outputs": [],
   "source": [
    "question1=\"What is Tom's profession?\"\n",
    "\n",
    "answer1=openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=question1,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(answer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1794065",
   "metadata": {
    "id": "f1794065"
   },
   "source": [
    "## Custom Completion for question 1\n",
    "\n",
    "Asks the same question using context-aware prompt generation. Allows comparison with the basic model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7a093b",
   "metadata": {
    "id": "bd7a093b"
   },
   "outputs": [],
   "source": [
    "custom_chatbot_anwer1=answer_question(question1, df)\n",
    "print(custom_chatbot_anwer1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86e37c",
   "metadata": {
    "id": "6e86e37c"
   },
   "source": [
    "## Basic completion for question 2\n",
    "\n",
    "Asks a basic question with no custom context. This shows how the model performs without dataset guidance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f646989",
   "metadata": {
    "id": "6f646989"
   },
   "outputs": [],
   "source": [
    "question2=\"In what setting does Thomas's story take place?\"\n",
    "\n",
    "answer2=openai.Completion.create(\n",
    "    model=\"gpt-3.5-turbo-instruct\",\n",
    "    prompt=question2,\n",
    "    max_tokens=150\n",
    ")[\"choices\"][0][\"text\"].strip()\n",
    "print(answer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "R7NWEkNp85m9",
   "metadata": {
    "id": "R7NWEkNp85m9"
   },
   "source": [
    "## Custom Completion for question 2\n",
    "\n",
    "Asks the same question using context-aware prompt generation. Allows comparison with the basic model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c07a54",
   "metadata": {
    "id": "11c07a54"
   },
   "outputs": [],
   "source": [
    "custom_chatbot_anwer2=answer_question(question2, df)\n",
    "print(custom_chatbot_anwer2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0NgNCuOJ9pFm",
   "metadata": {
    "id": "0NgNCuOJ9pFm"
   },
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd2fd78",
   "metadata": {
    "id": "dbd2fd78"
   },
   "source": [
    "## Scenario Demonstration\n",
    "\n",
    "I tested this model with two sample questions which shows the output\n",
    "* Using a basic prompt (no custom context)\n",
    "* Using our custom prompt with dataset context\n",
    "\n",
    "This helps evaluate the value of tailored context in improving answer accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d598b133",
   "metadata": {
    "id": "d598b133"
   },
   "source": [
    "## Results and Discussion\n",
    "\n",
    "I observed that the answers generated using custom prompts that included dataset-specific context were more accurate and relevant than those generated by basic prompts.\n",
    "\n",
    "This highlights the advantage of embedding-based similarity search combined with prompt engineering for building specialized chatbots."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
